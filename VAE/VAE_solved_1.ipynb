{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfedad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n",
      "2.3.1\n",
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e72777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n",
      "2.3.1\n",
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n",
    "print(cv2.__version__)\n",
    "\n",
    "def load_data(path,size):\n",
    "    high_res_images = []\n",
    "    low_res_images = []\n",
    "    count = 0\n",
    "    for (dirname, _, filenames_high_res),(dirname, _, filenames_low_res) in zip(os.walk(path+'HIGH'),os.walk(path+'LOW')):\n",
    "        filenames_high_res.sort()\n",
    "        filenames_low_res.sort()\n",
    "        \n",
    "        for filename_high,filename_low in zip(filenames_high_res,filenames_low_res):\n",
    "            #faulty image\n",
    "            \n",
    "            try:\n",
    "                count+=1\n",
    "                img = cv2.imread(os.path.join(dirname, filename_high))\n",
    "                img_2 = cv2.imread(os.path.join(dirname, filename_low))\n",
    "                # resizing the images\n",
    "                width=int(img.shape[1])\n",
    "                height=int(img.shape[0])\n",
    "                \n",
    "                width_2= int(img_2.shape[1])\n",
    "                height_2=int(img_2.shape[0]) \n",
    "            \n",
    "                if width!= 256 and height !=256:\n",
    "                    dsize=(256,256)\n",
    "                    img=cv2.resize(img,dsize)\n",
    "                  \n",
    "                if width_2!= 256 and height_2 !=256:\n",
    "                    dsize=(256,256)\n",
    "                    img_2=cv2.resize(img_2,dsize)\n",
    "                \n",
    "                img = process_image(img)\n",
    "                img_2 = process_image(img_2)\n",
    "                high_res_images.append(img)\n",
    "                low_res_images.append(img_2)\n",
    "            except:\n",
    "                continue\n",
    "            if count >size:\n",
    "                break\n",
    "    \n",
    "    # zero-mean and zero-center the standard deviations ....\n",
    "    low_res_array = np.array(low_res_images)\n",
    "    #mean_centered_low_res = (low_res_array - low_res_array.mean())/low_res_array.std()\n",
    "    \n",
    "    # zero-mean and zero-center the standard deviations .....\n",
    "    high_res_array = np.array(high_res_images)\n",
    "    #mean_centered_high_res = (high_res_array - high_res_array.mean())/high_res_array.std()\n",
    "    \n",
    "    \n",
    "    #return mean_centered_low_res, mean_centered_high_res\n",
    "    return low_res_array,high_res_array\n",
    "\n",
    "def process_image(image):\n",
    "    return image/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = ''\n",
    "train_x, train_y =  load_data(path=base_dir+'Training/',size=7000)\n",
    "val_x, val_y = load_data(path=base_dir+'Validationing/',size=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35e094a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-04aa4f6d450a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b82645",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d675436",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a16f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be643fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuploaded_model = tf.keras.models.load_model('VAE_3_DropOut_256_final_MSE_KL_VGG_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = reuploaded_model.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[50])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[50])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][50])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[50])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[50])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[2][50])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[100])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[100])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][100])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159645c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[150])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[150])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[2][150])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[150])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[150])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][150])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec73b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[250])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[250])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][250])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c807a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[250])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[250])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[2][250])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[20])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[20])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][20])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a87ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[20])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[20])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[2][20])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-input and output models for probing latent space and for analysing high resolution output\n",
    "## Convoluional Variational Autoencoderds for Super Resolution\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# This class creates a sample given a mean and variance - assuming a Gaussian distribution\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "latent_dim = 64\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "encoder_inputs = keras.Input(shape=input_shape,name='img_input')\n",
    "#dim_encoding_2 = keras.Input(shape = (latent_dim),name='dim_encode')\n",
    "#latent_ratio = keras.Input(shape = (1),name='latency_ratio')\n",
    "l1 = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(encoder_inputs) # input shape 256,256,3\n",
    "l2 = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l1) #input shape 128,128,64\n",
    "#l2 = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001)(l2)\n",
    "l2 = Dropout(0.3)(l2)\n",
    "l3 = layers.Conv2D(128, 3, activation=\"relu\", strides=1, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l2) # 64,64,64\n",
    "l4 = layers.Flatten()(l3)\n",
    "l5 = layers.Dense(256, activation=\"relu\")(l4) # best if this dimension is greater than the latent_dim simension\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(l5)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(l5)\n",
    "\n",
    "\n",
    "#z_mean = dim_encoding_2*(1-latent_ratio) + z_mean*latent_ratio\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "#encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "#encoder.summary()\n",
    "\n",
    "#latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "#l6 = layers.Dense(64 * 64 * 64, activation=\"relu\")(latent_inputs)\n",
    "l6 = layers.Dense(64 * 64 * 64, activation=\"relu\")(z)\n",
    "l7 = layers.Reshape((64, 64, 64))(l6) #shape is 64,64,64\n",
    "l8 = layers.add([l2,l7])\n",
    "l9 = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l8)\n",
    "#l9 = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001)(l9)\n",
    "l9 = Dropout(0.3)(l9)\n",
    "l10 = layers.add([l1,l9])\n",
    "l11 = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=1, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l10) #256,256,64\n",
    "\n",
    "l12 = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l11) #256,256,32\n",
    "l12 = Dropout(0.3)(l12)\n",
    "#l12 = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001)(l12)\n",
    "l13 = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l12) #512,512,32\n",
    "l13 = Dropout(0.3)(l13)\n",
    "#l13 = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001)(l13)\n",
    "#decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"relu\", padding=\"same\")(l13) #256,256,3\n",
    "l14 = layers.Conv2D(3, 3, activation=\"relu\", strides=1,padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l13) #512,512,3\n",
    "decoder_outputs = layers.Conv2D(3, 3, activation=\"relu\", strides=2,padding=\"same\",bias_initializer='zeros',\n",
    "                activity_regularizer=regularizers.l1(10e-10),kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(seed=None,dtype=tf.dtypes.float32))(l14) #256,256,3\n",
    "#decoder_outputs = tf.slice(decoder_outputs, begin=[0, 0, 0, 0], size=[tf.shape(decoder_outputs)[0], 28, 28, 1])\n",
    "\n",
    "print(decoder_outputs.shape)\n",
    "decoder_3 = keras.Model([encoder_inputs],[z_mean,z_log_var,l14,decoder_outputs], name=\"decoder\")\n",
    "decoder_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The fully convolutional encoder-decoder network and then creating a new loss function with the VGG_19 loss\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "vgg_19 = tf.keras.applications.VGG19(include_top=False,weights='imagenet',input_shape=(256,256,3),pooling=None)\n",
    "vgg_19.summary()\n",
    "\n",
    "#vgg_19 = Model(inputs= vgg_19.input,outputs=vgg_19.get_layer('block5_conv4').output)\n",
    "vgg_19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VGG loss implementation\n",
    "### Modifying the train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, decoder,vgg_loss, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.decoder = decoder\n",
    "        self.vgg_loss = vgg_loss\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.vgg_loss_tracker = keras.metrics.Mean(name=\"vgg_loss\")\n",
    "        \n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(name=\"val_reconstruction_loss\")\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "        self.val_vgg_loss_tracker = keras.metrics.Mean(name=\"val_vgg_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.vgg_loss_tracker\n",
    "        ]\n",
    "    \n",
    "    def call(self, data, training = False):\n",
    "#         image = data['img_input']\n",
    "#         dim_encoding = data['dim_encode']\n",
    "#         latency = data['latency_ratio']\n",
    "        if training:\n",
    "            z_mean,z_log_var,_,reconstruction = self.decoder(data) # data is a tuple of encoder_inputs,dim_encoding_2,latent_ratio\n",
    "        else:\n",
    "            z_mean,z_log_var,d512,reconstruction= self.decoder(data)\n",
    "            return (z_mean,z_log_var,d512,reconstruction)\n",
    "        return reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            z_mean,z_log_var,_,reconstruction = self.decoder(data) # data is a tuple of encoder_inputs,dim_encoding_2,latent_ratio\n",
    "            \n",
    "            reconstruction_loss = tf.keras.losses.MSE(data,reconstruction)\n",
    "            \n",
    "            ##VGG loss\n",
    "            vgg_losss = self.vgg_loss(data)\n",
    "            recon = self.vgg_loss(reconstruction)\n",
    "            vgg_loss = 3e-5*tf.reduce_mean(tf.keras.losses.MSE(vgg_losss,recon))\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            #if reconstruction_loss/kl_loss\n",
    "            total_loss = reconstruction_loss + kl_loss*1e-6 + vgg_loss\n",
    "        grads = tape.gradient(total_loss, self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.vgg_loss_tracker.update_state(vgg_loss)\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"vgg_loss\": self.vgg_loss_tracker.result()\n",
    "        }\n",
    "    \n",
    "    def test_step(self,data):\n",
    "        x,y = data\n",
    "        z_mean,z_log_var,d512,y_pred = self(x,training=False)\n",
    "        #print(y_pred)\n",
    "        #z_mean,z_log_var,_,reconstruction = self.decoder(data)\n",
    "        reconstruction_loss = tf.keras.losses.MSE(data,y_pred)\n",
    "        ##VGG loss\n",
    "        vgg_losss = self.vgg_loss(data)\n",
    "        recon = self.vgg_loss(y_pred)\n",
    "        vgg_loss = 3e-5*tf.reduce_mean(tf.keras.losses.MSE(vgg_losss,recon))\n",
    "            \n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss*1e2 + vgg_loss\n",
    "        self.val_total_loss_tracker.update_state(total_loss)\n",
    "        self.val_reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.val_kl_loss_tracker.update_state(kl_loss)\n",
    "        self.val_vgg_loss_tracker.update_state(vgg_loss)\n",
    "        \n",
    "        return {\"total_loss\": self.val_total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.val_reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.val_kl_loss_tracker.result(),\n",
    "            \"vgg_loss\": self.val_vgg_loss_tracker.result()\n",
    "            }\n",
    "    \n",
    "    #def predict_step(self,data):\n",
    "        \n",
    "    \n",
    "\n",
    "model_12 = VAE(decoder_3,vgg_19)\n",
    "model_12.compile(optimizer=keras.optimizers.Adam(1e-4),loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f980e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modifying the train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(name=\"val_reconstruction_loss\")\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def call(self, data, training = False):\n",
    "#         image = data['img_input']\n",
    "#         dim_encoding = data['dim_encode']\n",
    "#         latency = data['latency_ratio']\n",
    "        if training:\n",
    "            z_mean,z_log_var,_,reconstruction = self.decoder(data) # data is a tuple of encoder_inputs,dim_encoding_2,latent_ratio\n",
    "        else:\n",
    "            z_mean,z_log_var,d512,reconstruction= self.decoder(data)\n",
    "            return (z_mean,z_log_var,d512,reconstruction)\n",
    "        return reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "#             image = data['img_input']\n",
    "#             dim_encoding = data['dim_encode']\n",
    "#             latency = data['latency_ratio']\n",
    "            z_mean,z_log_var,_,reconstruction = self.decoder(data) # data is a tuple of encoder_inputs,dim_encoding_2,latent_ratio\n",
    "            \n",
    "            reconstruction_loss = tf.keras.losses.MSE(data,reconstruction)\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            #if reconstruction_loss/kl_loss\n",
    "            total_loss = reconstruction_loss + kl_loss*1e2\n",
    "        grads = tape.gradient(total_loss, self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self,data):\n",
    "        x,y = data\n",
    "        z_mean,z_log_var,d512,y_pred = self(x,training=False)\n",
    "        #print(y_pred)\n",
    "        #z_mean,z_log_var,_,reconstruction = self.decoder(data)\n",
    "        reconstruction_loss = tf.keras.losses.MSE(data,y_pred)\n",
    "            \n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss*1e2\n",
    "        self.val_total_loss_tracker.update_state(total_loss)\n",
    "        self.val_reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.val_kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\"total_loss\": self.val_total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.val_reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.val_kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    #def predict_step(self,data):\n",
    "        \n",
    "    \n",
    "\n",
    "model_7 = VAE(decoder_3)\n",
    "model_7.compile(optimizer=keras.optimizers.Adam(1e-4),loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_12= model_12.fit(train_x, train_y, validation_data =(val_x , val_y),epochs = 20, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dee367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_12.save('VAE_3_DropOut_256_final_MSE_KL_VGG_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xx = model_12.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_y = reuploaded_model.predict(np.expand_dims(train_x[800],axis=0))\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.plot(history_12.history['total_loss'][1:])\n",
    "plt.plot(history_12.history['reconstruction_loss'][1:])\n",
    "plt.plot(history_12.history['val_total_loss'][1:])\n",
    "plt.plot(history_12.history['val_reconstruction_loss'][1:])\n",
    "#plt.plot(history_5.history['kl_loss'][1:])\n",
    "plt.title('VAE Loss Curves KL+MSE+VGG Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['total train-loss', 'train reconstruction-loss', 'total_val_loss', 'total_val_reconstruction_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.plot(history_5.history['val_total_loss'][1:])\n",
    "plt.plot(history_5.history['val_reconstruction_loss'][1:])\n",
    "#plt.plot(history_5.history['val_kl_loss'][1:])\n",
    "plt.title('VAE Loss Curves KL+MSE Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['total train-loss', 'reconstruction-loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = model_7.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49493266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.save('VAE_3_DropOut_256_final_MSE_KL_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "modeling = keras.models.load_model('VAE_3_DropOut_256_final_MSE_KL')\n",
    "predict_y = modeling.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_y = model.predict(val_x)\n",
    "#plt.subplots(2,2,figsize=(15,15))\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[50])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[50])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[2][50])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62556fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[50])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[50])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][50])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6578a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[150])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[150])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][150])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[200])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[200])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][200])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef345",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[250])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[250])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][250])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d60ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,30))\n",
    "plt.figure(figsize=(40,40))\n",
    "ax1.imshow(val_x[10])\n",
    "ax1.title.set_text(\"low-res image \")\n",
    "ax2.imshow(val_y[10])\n",
    "ax2.title.set_text(\"high-res image \")\n",
    "ax3.imshow(predict_y[3][10])\n",
    "ax3.title.set_text(\"model's output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f21ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
